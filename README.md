# Neurodex Bot

A scalable Telegram trading bot built with TypeScript, featuring OpenOcean integration for cryptocurrency trading, functional programming approach, and robust error handling.

## Features

- TypeScript with strict type checking
- Modern ES2022 features
- Functional programming paradigm
- Robust error handling
- Environment configuration with Zod validation
- ESLint and Prettier for code quality
- Hot reloading during development
- OpenOcean integration for DEX trading

## Prerequisites

- Node.js (v18 or higher)
- pnpm
- A Telegram bot token (get it from [@BotFather](https://t.me/BotFather))
- QuickNode account with OpenOcean addon
- Make (for using the Makefile)
- Prisma (PostgreSQL) Database

## Setup

1. Clone the repository:
```bash
git clone https://github.com/axioma-ai-labs/neurodex-bot
cd neurodex-bot
```

2. Install dependencies:
```bash
make deps
```

3. Copy the example environment file and update it with your values:

```bash
cp .env.example .env
```

4. Configure your `.env` file with the required values. See `.env.example` for reference.

5. Generate Prisma client:

```bash
pnpm prisma generate --no-engine
```

6. Run the application:

```bash
make dev
```

------


## Makefile

The project includes a Makefile with common commands for development:

```bash
# Show available commands
make help
```

## Testing separate methods (OpenOcean, Wallet Generation, etc.)

### OpenOcean + QuickNode

You can test the OpenOcean integration directly using:

```bash
make run CMD="src/services/openocean/index.ts swap"
```

Available test commands:
- connection
- gas
- quote
- mev
- balance
- tokenList
- transaction
- swap

If not specified - all methods will be executed and run.

### Wallet Creation

**IMPORTANT**: Before running, uncomment code at the bottom.

```bash
make run CMD="scripts/create_test_wallet.ts"
```


## Development

Start the development server with hot reloading:
```bash
make dev
```

## Building and Running for Production

```bash
# Start the production server
pnpm run start
```

## Database

We use Prisma with PostgreSQL and Accelerate.

Basic usage is very simple:
1. All the database operations are in `src/services/db/`. Here we isolate logic of interacting with the database.
2. Definition of models is in `prisma/schema.prisma`. There you'll find migrations as well.
3. Database instance is in `src/services/db/prisma.ts`. Very minimalistic, so don't touch it.
4. If we change the models, we need to run `make prisma-migrate-dev` to update the migrations. It will automatically create a new migration file in `prisma/migrations` and apply it to the database. [More details below]

> [!IMPORTANT]
> We currently have only development database. Production one comes soon.

Example usage (script for testing db ops):

```bash
make run CMD="scripts/db-connection.ts"
```

### Prisma Client

This is the client for interacting with the database. It's generated by Prisma, so we don't have to write it manually. However, there're some important settings:

- The client is generated into `node_modules/.prisma/client`
- We need to copy it to the build stage in `Dockerfile`
- We need to generate the client after reinstalling dependencies

```bash
pnpm prisma generate --no-engine
```

### Encryption

We use `prisma-field-encryption` to encrypt the fields in the database. There're important things to remember:

- Always use `/// @encrypted` annotation to encrypt the field.
- Do not query encrypted fields!
- Store the encryption key in the environment variable

For more details - refer to the [prisma-field-encryption](https://github.com/47ng/prisma-field-encryption) repo.

### Migrations

Migrations are handled by Prisma, so we don't have to write them manually. Just change the models in `prisma/schema.prisma` and run:

```bash
make migrate
```

This will create a new migration file in `prisma/migrations` and apply it to the database.

### Backups

We don't have Pro plan atm, so we need to manually backup the database. This is very simple:

1. Run the tunnel:

```bash
pnpm ppg-tunnel --host 127.0.0.1  --port 5432
```

2. Run the backup:

```bash
PGSSLMODE=disable \
pg_dump \
  -h 127.0.0.1 \
  -p 5432 \
  -Fc \
  -v \
  -d postgres \
  -f ./mydatabase.bak \
&& echo "-complete-"
```

Note, that you need to have `pg_dump` (comes with PostgreSQL) installed.

> TODO: Add simple postgres docker container to run the backup.

---

# Learnings

Summary of learnings for Neurodex trading bot. Kinda super important!

### üîê Security Learnings

* **Use AES-256-GCM instead of AES-256-CBC**
  GCM provides both confidentiality and integrity (auth tag) and is resistant to padding oracle attacks.

* **Always use a random Initialization Vector (IV)**
  For GCM mode, a 12-byte IV (`crypto.randomBytes(12)`) is recommended and must be unique per encryption.

* **Derive encryption key using SHA-256**
  Use `crypto.createHash('sha256').update(secret).digest()` to produce a fixed-length 256-bit key from your secret.

* **Store IV and Auth Tag with encrypted data**
  Concatenate them in a predictable format (e.g., `iv:authTag:encrypted`) so they can be reused during decryption.

* **Keep private keys encrypted at all times**
  Never store or log raw private keys. Decrypt them only when absolutely needed and clear from memory after use.

* **Use strong, unpredictable secrets**
  Your `encryptionKey` should be at least 32 characters long, random, and not reused across environments.

* **Implement proper key rotation (for the future)**
  Design your system to support periodic key changes without data loss. Use versioned encryption keys if needed.

* **Limit exposure window**
  Auto-delete private keys within a short time (e.g., 5 minutes).

* **Upscale the database**
  In the beginning we can use any db, but in the future we might face scaling issues. E.g. 20 active users and each user makes some settings request. This converts to 20 requests per second. 
  The solution is to upscale the database, and most importantly, allow concurrent connections. This is not so easy as it sounds, so using 3-rd party service like Accelerate is a good idea. 
  We already activated it in Development environment, so this should be a good example to follow.

* **Onchain is fucking hard**
  Finding is quite simple: onchain staff is poorly documented, relatively complex and not very-well maintained. Unfortunately third-party providers also don't help much...

  Very important finding is, that third-party providers just prepare transactions/contracts/etc., but the execution is done on the node via viem (or similar library).

  Also how openocean contract is being found from the viem is still a holy mystery...